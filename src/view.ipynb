{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/zhenduow/miniconda3/envs/py385/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "2024-01-31 23:29:25.953343: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-31 23:29:26.002211: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 23:29:26.805730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package wordnet to /home/zhenduow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/zhenduow/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/zhenduow/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import json\n",
    "import rouge\n",
    "rs = rouge.Rouge()\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "pos_tagger = spacy.load('en_core_web_sm')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.combine([\"meteor\", \"rouge\"])\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bs = evaluate.load(\"bertscore\")\n",
    "\n",
    "idk_list = [\n",
    "    'i dont know',\n",
    "    'i do not know',\n",
    "    'im not sure',\n",
    "    'i am not sure',\n",
    "    'unsure',\n",
    "    'possibly',\n",
    "    'this is not related to my search'\n",
    "    ]\n",
    "\n",
    "negation_list = [\n",
    "    'no',\n",
    "    'not',\n",
    "    'none',\n",
    "    'isnt',\n",
    "    'isn\\'t',\n",
    "    'dont',\n",
    "    'don\\'t',\n",
    "    ]\n",
    "\n",
    "def read_dialog_output(dialog_output):\n",
    "    import subprocess\n",
    "    result = subprocess.run(['jq','.metrics', dialog_output], stdout=subprocess.PIPE)\n",
    "    res_dict = json.loads(result.stdout)\n",
    "    return res_dict['ndcg@1']['mean'], \\\n",
    "    res_dict['ndcg@5']['mean'], \\\n",
    "    res_dict['ndcg@20']['mean'], \\\n",
    "    res_dict['p@1']['mean'], \\\n",
    "    res_dict['mrr']['mean']\n",
    "        \n",
    "def type_answer(answer):\n",
    "    answer = re.sub(',', '', answer)\n",
    "    if answer in idk_list:\n",
    "        return 'idk'\n",
    "    elif 'yes' in answer.split()[:3]:\n",
    "        return 'yes'\n",
    "    elif any([w in answer.split()[:3] for w in negation_list]):\n",
    "        return 'no'\n",
    "    else:\n",
    "        return 'open'\n",
    "\n",
    "def compute_metrics(refs, cands):\n",
    "    result = metric.compute(predictions=cands, references=refs, use_stemmer=True)\n",
    "    bleu3 = bleu.compute(predictions=cands, references=[[r] for r in refs], max_order = 3)\n",
    "    bleu4 = bleu.compute(predictions=cands, references=[[r] for r in refs], max_order = 4)\n",
    "    bertscore = bs.compute(predictions=cands, references=refs, lang = 'en')\n",
    "    result['bleu3'] = bleu3['bleu']\n",
    "    result['bleu4'] = bleu4['bleu']\n",
    "    result['bertscore_precision'] = np.mean(bertscore['precision'])\n",
    "    result['bertscore_recall'] = np.mean(bertscore['recall'])\n",
    "    result['bertscore_f1'] = np.mean(bertscore['f1'])\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items() }\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_from_output(model_output, column_name, dialog_output):\n",
    "    model_output_data = pd.read_csv(model_output)\n",
    "    model_output_data = model_output_data.dropna(subset=['reference'])\n",
    "\n",
    "    cand_type, ref_type = [], []\n",
    "\n",
    "    for iter, row in model_output_data.iterrows():\n",
    "        ref = row['reference'].lower()\n",
    "        try:\n",
    "            candidate = row[column_name].lower()\n",
    "        except:\n",
    "            candidate = 'no'\n",
    "        #print(iter, candidate)\n",
    "        cand_type.append(type_answer(candidate))\n",
    "        ref_type.append(type_answer(ref))\n",
    "\n",
    "        t = f1_score(ref_type, cand_type, average = 'macro')\n",
    "        #t = accuracy_score(ref_type, cand_type)\n",
    "\n",
    "    model_output_data.to_csv(model_output)\n",
    "\n",
    "    ndcg1, ndcg5, ndcg20, p1, mrr = read_dialog_output(dialog_output)\n",
    "\n",
    "    output_df = pd.read_csv(model_output)\n",
    "\n",
    "    result = compute_metrics(refs = output_df['reference'].values.tolist(), cands = output_df[column_name].values.tolist())\n",
    "\n",
    "    return result['bleu3'], result['bleu4'], result['rougeL'], result['meteor'], result['bertscore_f1'], t, ndcg1, ndcg5, ndcg20, p1, mrr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qulac Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|         qulac          |                 generation similarity                |            retrieval performance            |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|MODEL                   | BLEU3    BLEU4    ROUGE-L  METEOR   BERT-F1  TYPE-F1 | nDCG@1   nDCG@5   nDCG@20  P@1      MRR     |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT3.5-0s               | 10.34    7.48     27.9     34.89    87.63    43.13   | 0.2045   0.2012   0.1829   0.267    0.3755  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT3.5-3s               | 14.02    10.39    30.14    31.71    87.92    43.73   | 0.1966   0.1938   0.1776   0.2543   0.363   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT4-0s                 | 15.44    11.59    34.18    38.59    88.91    46.85   | 0.2105   0.2072   0.19     0.2728   0.383   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT4-3s                 | 17.79    13.55    34.96    36.45    89.18    45.36   | 0.214    0.2073   0.1886   0.2757   0.3841  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Flan-xxl-0s             | 0.66     0.47     22.85    10.71    85.24    44.44   | 0.1665   0.1742   0.1632   0.2221   0.3287  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Flan-xxl-3s             | 0.13     0.1      23.94    10.65    85.31    49.27   | 0.1671   0.1726   0.1616   0.2229   0.3274  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|LlaMa2-0s               | 10.74    7.93     22.46    21.79    86.54    22.41   | 0.1907   0.1857   0.1714   0.2507   0.3526  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|LlaMa2-3s               | 3.88     2.92     18.21    11.98    85.68    27.74   | 0.1722   0.1765   0.1643   0.2282   0.3345  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|T5-qulac                | 23.68    19.0     40.81    43.15    89.76    34.07   | 0.215    0.2088   0.1872   0.2789   0.388   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|UnifiedQA-qulac         | 23.56    18.83    40.89    43.3     89.66    41.34   | 0.2148   0.2101   0.1882   0.2788   0.3904  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|UnifiedQA-roberta       | 24.44    19.61    41.66    43.49    89.74    43.28   | 0.2137   0.2098   0.1888   0.2774   0.39    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|T5-COOP                 | 27.76    22.1     47.75    45.37    92.04    41.5    | 0.2061   0.2029   0.1816   0.268    0.3773  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|T5-short                | 23.25    0.0      42.44    20.91    94.3     24.52   | 0.2061   0.2029   0.1816   0.268    0.3773  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|T5-long                 | 27.88    22.43    47.35    50.49    91.32    26.04   | 0.2061   0.2029   0.1816   0.268    0.3773  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|RU-COOP                 | 30.06    24.29    50.52    46.7     92.08    50.11   | 0.2032   0.2024   0.1816   0.2648   0.3759  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Query only              | -        -        -        -        -        -       | 0.1329   0.1461   0.1525   0.19     0.2938  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Reference               | 100.0    100.0    100.0    93.11    100.0    100.0   | 0.1982   0.1946   0.1776   0.2593   0.3674  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Copy Intent             | 17.34    13.77    30.7     28.18    87.69    8.42    | 0.2161   0.2123   0.1931   0.2831   0.3913  |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|         qulac          |                 generation similarity                |            retrieval performance            |\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL','BLEU3','BLEU4', 'ROUGE-L','METEOR', 'BERT-F1', 'TYPE-F1', 'nDCG@1', 'nDCG@5', 'nDCG@20', 'P@1', 'MRR'))\n",
    "\n",
    "\n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt35-qulac-0shot.csv', 'candidate', 'output/gpt35-qulac-0s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT3.5-0s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    " \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt35-qulac-3shot.csv', 'candidate', 'output/gpt35-qulac-3s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT3.5-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt4-qulac-0shot.csv', 'candidate', 'output/gpt4-qulac-0s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT4-0s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))  \n",
    " \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt4-qulac-3shot.csv', 'candidate', 'output/gpt4-qulac-3s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT4-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/flan-qulac-0shot.csv', 'candidate', 'output/flan-qulac-0shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Flan-xxl-0s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))          \n",
    "\n",
    "\n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/flan-qulac-3shot.csv', 'candidate', 'output/flan-qulac-3shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Flan-xxl-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))          \n",
    "        \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/llama2-qulac-0shot.csv', 'candidate', 'output/llama2-qulac-0shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('LlaMa2-0s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))                                                                                         \n",
    "\n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/llama2-qulac-3shot.csv', 'candidate', 'output/llama2-qulac-3shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('LlaMa2-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))                                                                                                                                                                    \n",
    "                                                                                        \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-qulac-30.csv', 'candidate', 'output/t5-small-qulac-30.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-qulac', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "                                                                                        \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/unifiedqa-small-qulac-30.csv', 'candidate', 'output/u-qulac-30.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('UnifiedQA-qulac', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "                                                                                        \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/unifiedqa-small-qulac-roberta.csv', 'candidate', 'output/unifiedqa-small-qulac-roberta.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('UnifiedQA-roberta', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "                                                                                        \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-qulac-30-all.csv', 'candidate', 'output/t-qulac-all.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-COOP', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-qulac-short.csv', 'candidate', 'output/t-qulac-all.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-short', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-qulac-long.csv', 'candidate', 'output/t-qulac-all.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-long', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/unifiedqa-small-qulac-roberta-all.csv', 'candidate', 'output/u-qulac-all.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('RU-COOP', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-qulac-30.csv', 'question', 'output/query-qulac.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Query only', \n",
    "                                                            \"-\", \"-\", \"-\", \"-\", \"-\", \"-\",\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "                                                            \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-qulac-30.csv', 'reference', 'output/human-qulac.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Reference', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "                                                                                        \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-qulac-30.csv', 'facet', 'output/copy-test.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Copy Intent', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClariQ Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|         clariq         |                 generation similarity                |            retrieval performance            |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|MODEL                   | BLEU3    BLEU4    ROUGE-L  METEOR   BERT-F1  TYPE-F1 | nDCG@1   nDCG@5   nDCG@20  P@1      MRR     |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT3.5                  | 9.68     6.96     26.88    34.62    87.3     43.77   | 0.1399   0.1303   0.1127   0.1651   0.2393  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT3.5-3s               | 12.17    8.78     28.44    30.43    87.56    43.82   | 0.1342   0.1243   0.1087   0.1567   0.2326  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT4                    | 13.77    10.17    32.5     37.08    88.46    47.04   | 0.1465   0.1334   0.1175   0.1701   0.2438  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|GPT4-3s                 | 16.42    12.21    34.08    35.8     88.82    45.74   | 0.1412   0.1284   0.1148   0.1654   0.2398  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Flan-xxl-0s             | 0.4      0.32     22.51    10.33    85.09    45.14   | 0.1307   0.1226   0.1063   0.1541   0.2282  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Flan-xxl-3s             | 0.12     0.09     23.21    10.13    85.11    47.37   | 0.128    0.1208   0.104    0.1505   0.2254  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|LLaMa2                  | 5.99     3.97     19.41    17.9     85.53    22.63   | 0.1382   0.1292   0.1113   0.1621   0.2355  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|LLaMa2                  | 1.04     0.78     16.7     9.04     85.15    33.77   | 0.1308   0.1204   0.1038   0.1532   0.226   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|T5-clariq               | 24.3     19.51    40.97    43.32    89.61    36.64   | 0.1497   0.1335   0.1182   0.1756   0.2485  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|UnifiedQA-clariq        | 24.32    19.41    41.63    43.58    89.55    45.9    | 0.1476   0.1349   0.1192   0.1704   0.2471  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|UnifiedQA-clariq-roberta| 25.24    20.19    42.08    43.05    89.6     46.27   | 0.1488   0.1364   0.1194   0.1722   0.2484  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|T5-COOP                 | 29.11    23.43    48.09    45.44    91.72    41.76   | 0.1472   0.1328   0.1165   0.1725   0.2463  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|RU-COOP                 | 30.39    24.31    49.96    45.6     91.78    53.41   | 0.1409   0.133    0.1164   0.1625   0.2422  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Query only              | -        -        -        -        -        -       | 0.1184   0.1087   0.0894   0.1316   0.2021  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Reference               | 100.0    100.0    100.0    93.43    100.0    100.0   | 0.1387   0.1272   0.1121   0.1629   0.2377  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "|Copy Intent             | 18.11    14.38    31.72    29.86    87.59    8.71    | 0.1494   0.1369   0.1217   0.1749   0.2497  |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|         clariq         |                 generation similarity                |            retrieval performance            |\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL','BLEU3','BLEU4', 'ROUGE-L','METEOR', 'BERT-F1', 'TYPE-F1', 'nDCG@1', 'nDCG@5', 'nDCG@20', 'P@1', 'MRR'))\n",
    "                                                                                \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt35-clariq-0shot.csv', 'candidate', 'output/gpt35-clariq-0s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT3.5', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))  \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt35-clariq-3shot.csv', 'candidate', 'output/gpt35-clariq-3s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT3.5-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))    \n",
    "                                                                                                                                                           \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt4-clariq-0shot.csv', 'candidate', 'output/gpt4-clariq-0s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT4', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "                                                                                                                                                                                                                                       \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/gpt4-clariq-3shot.csv', 'candidate', 'output/gpt4-clariq-3s.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('GPT4-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/flan-clariq-0shot.csv', 'candidate', 'output/flan-clariq-0shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Flan-xxl-0s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))          \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/flan-clariq-3shot.csv', 'candidate', 'output/flan-clariq-3shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Flan-xxl-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))          \n",
    "                       \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/llama2-clariq-0shot.csv', 'candidate', 'output/llama2-clariq-0shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('LLaMa2-0s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/llama2-clariq-3shot.csv', 'candidate', 'output/llama2-clariq-3shot.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('LLaMa2-3s', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/t5-small-clariq-30.csv', 'candidate', 'output/t5-small-clariq-30.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-clariq', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))    \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/unifiedqa-small-clariq-30.csv', 'candidate', 'output/unifiedqa-small-clariq-30.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('UnifiedQA-clariq', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))    \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/unifiedqa-small-clariq-roberta.csv', 'candidate', 'output/unifiedqa-small-clariq-roberta.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('UnifiedQA-clariq-roberta', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))    \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/t5-small-clariq-30-all.csv', 'candidate', 'output/t-clariq-all.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-COOP', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr  = evaluate_from_output('output/unifiedqa-small-clariq-roberta-all.csv', 'candidate', 'output/r-clariq-all.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('RU-COOP', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))    \n",
    "                                                            \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/t5-small-clariq-30.csv', 'question', 'output/query-clariq.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Query only', \n",
    "                                                            \"-\", \"-\", \"-\", \"-\", \"-\", \"-\",\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/t5-small-clariq-30.csv', 'reference', 'output/reference-clariq.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Reference', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))    \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/t5-small-clariq-30.csv', 'facet', 'output/copy-clariq.csv.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Copy Intent', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "device = T.device(\"cuda\")\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"output/roberta-qulac/checkpoint-102\")\n",
    "\n",
    "\n",
    "\n",
    "local_dir = \"./output/t5-small-qulac/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_dir).cuda()\n",
    "df = pd.read_csv('qulac_test.csv')\n",
    "\n",
    "for iter, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    test_tokenized = tokenizer.encode_plus(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "    test_input_ids = test_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "\n",
    "    clf_inputs = roberta_tokenizer(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "\n",
    "    with T.no_grad():\n",
    "        logits = roberta_model(**clf_inputs).logits\n",
    "\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    prefix = ''\n",
    "    if predicted_class_id == 3 : # 3: yes\n",
    "        prefix = 'yes'\n",
    "    elif predicted_class_id == 1: #1: no\n",
    "        prefix = 'no'\n",
    "    elif predicted_class_id == 0: # 0: idk\n",
    "        prefix = 'i dont know'\n",
    "    else:\n",
    "        prefix = ''\n",
    "\n",
    "    tokenized_decoder_input = tokenizer.encode_plus(prefix, return_tensors=\"pt\")\n",
    "    decoder_input_ids = tokenized_decoder_input[\"input_ids\"].to(device)[0][0].item()\n",
    "    decoder_input_ids = T.tensor([[0, decoder_input_ids]]).to(device)\n",
    "    \n",
    "    df.at[iter, 'decoder-input'] = prefix\n",
    "    inputs = tokenizer(row['unifiedqa-question'], max_length=128, padding=True, truncation=True)\n",
    "    inputs['decoder_input_ids'] = decoder_input_ids\n",
    "    print(prefix)\n",
    "    print(model(inputs))\n",
    "\n",
    "    '''\n",
    "    tokenized_decoder_input = tokenizer.encode_plus(prefix, return_tensors=\"pt\")\n",
    "    decoder_input_ids = tokenized_decoder_input[\"input_ids\"].to(device)[0][0].item()\n",
    "    decoder_input_ids = T.tensor([[0, decoder_input_ids]]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    if prefix == '':\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            decoder_input_ids = decoder_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(beam_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "\n",
    "    df.at[iter, 'candidate'] = decoded_output.strip()\n",
    "    df.at[iter, 'facet_desc'] = df.at[iter, 'facet_desc'].lower()\n",
    "    df.at[iter, 'question'] = df.at[iter, 'question'].lower()\n",
    "\n",
    "    output = df[['facet_desc','question','answer','candidate']].copy(deep=True)\n",
    "    output.columns = ['facet', 'question', 'reference','candidate']\n",
    "    output.to_csv('qulac_test.csv')\n",
    "    '''\n",
    "df.to_csv('qulac_test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "device = T.device(\"cuda\")\n",
    "\n",
    "local_dir = \"./output/unifiedqa-trainer-small-qulac-30/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_dir).cuda()\n",
    "\n",
    "df = pd.read_csv('qulac_test.csv')\n",
    "\n",
    "for iter, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    test_tokenized = tokenizer.encode_plus(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "    test_input_ids = test_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "    answer_type = row['answer-type']\n",
    "\n",
    "    ''' \n",
    "    prefix = ''\n",
    "    if answer_type == 'yes' or answer_type == 'no':\n",
    "        prefix = answer_type\n",
    "    elif answer_type == 'open':\n",
    "        prefix = ''\n",
    "    else:\n",
    "        prefix = 'i dont know'\n",
    "    '''\n",
    "\n",
    "    prefix = row['answer'].split()[0]\n",
    "\n",
    "    model.eval()\n",
    "    beam_output = model.generate(\n",
    "        input_ids=test_input_ids,\n",
    "        max_length=96,\n",
    "        early_stopping=True,\n",
    "        num_beams=10,\n",
    "        top_k=50, \n",
    "        top_p=0.9, \n",
    "        num_return_sequences=10,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    decoded_output = [tokenizer.decode(bo, skip_special_tokens=True,clean_up_tokenization_spaces=True) for bo in beam_output]\n",
    "\n",
    "    if prefix == 'yes' or prefix == 'no':\n",
    "        filtered_output = [d for d in decoded_output if (len(d)>0) and (d.split()[0] == prefix) ]\n",
    "        gen = filtered_output[0] if len(filtered_output) > 0 else decoded_output[0]\n",
    "    else:\n",
    "        gen = decoded_output[0]\n",
    "\n",
    "    df.at[iter, 'candidate'] = gen.strip()\n",
    "\n",
    "    output = df[['facet_desc','question','answer','candidate']].copy(deep=True)\n",
    "    output.columns = ['facet', 'question', 'reference','candidate']\n",
    "    output.to_csv('output/unifiedqa-small-qulac-oracle-type.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "device = T.device(\"cuda\")\n",
    "\n",
    "local_dir = \"./output/unifiedqa-trainer-small-qulac-30/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_dir).cuda()\n",
    "\n",
    "df = pd.read_csv('qulac_test.csv')\n",
    "\n",
    "for iter, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    test_tokenized = tokenizer.encode_plus(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "    test_input_ids = test_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "    answer_type = row['answer-type']\n",
    "\n",
    "    \n",
    "    prefix = ''\n",
    "    if answer_type == 'yes' or answer_type == 'no':\n",
    "        prefix = answer_type\n",
    "    elif answer_type == 'open':\n",
    "        prefix = ''\n",
    "    else:\n",
    "        prefix = 'i dont know'\n",
    "    \n",
    "    #prefix = row['answer'].split()[0]\n",
    "    if prefix == \"\":\n",
    "        model.eval()\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    else:\n",
    "        tokenized_decoder_input = tokenizer.encode_plus(prefix, return_tensors=\"pt\")\n",
    "        tokenized_prefix = tokenized_decoder_input[\"input_ids\"].to(device)\n",
    "        len_tokenized_prefix = T.tensor(len(tokenized_prefix[0]), dtype=T.long, device=device)\n",
    "        decoder_input_ids =  T.tensor([[0]]).to(device)\n",
    "        for token in range(len_tokenized_prefix-1):\n",
    "            decoder_input_ids = T.cat((decoder_input_ids, T.tensor([[tokenized_prefix[0][token]]]).to(device)), 1)\n",
    "\n",
    "        model.eval()\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            decoder_input_ids = decoder_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(beam_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "\n",
    "    df.at[iter, 'candidate'] = decoded_output.strip()\n",
    "\n",
    "    output = df[['facet_desc','question','answer','candidate']].copy(deep=True)\n",
    "    output.columns = ['facet', 'question', 'reference','candidate']\n",
    "    output.to_csv('output/unifiedqa-small-qulac-oracle-gen.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "device = T.device(\"cuda\")\n",
    "\n",
    "local_dir = \"./output/unifiedqa-trainer-small-qulac-30/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_dir).cuda()\n",
    "\n",
    "\n",
    "df = pd.read_csv('qulac_dev.csv')\n",
    "\n",
    "input_ids = tokenizer(df.at[1,'unifiedqa-question'], return_tensors=\"pt\").input_ids.to(device)\n",
    "labels = tokenizer(df.at[1,'answer'], return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model(input_ids=input_ids, labels = labels)\n",
    "\n",
    "logits = outputs['logits']\n",
    "bsz, length, vsz = logits.shape\n",
    "\n",
    "print(\"logits size\", bsz, length, vsz)\n",
    "\n",
    "print(\"labels size\", labels.shape)\n",
    "\n",
    "\n",
    "category_labels = T.zeros(bsz)\n",
    "\n",
    "print(\"c_l size\", category_labels.shape)\n",
    "print(\"c_l\", category_labels)\n",
    "category_labels[5][0] = 1\n",
    "print(\"c_l\", category_labels)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "p_idk = 0\n",
    "for utterance in idk_list:\n",
    "    tokenized_u = tokenizer(utterance, return_tensors=\"pt\").input_ids[0]\n",
    "    p_utterance = 1\n",
    "    for i in range(min(outputs['logits'].size(1), len(tokenized_u[0]))):\n",
    "        idx = tokenized_u[i]\n",
    "        p_utterance *= T.exp(outputs['logits'][0][i][idx])\n",
    "    p_idk += p_utterance\n",
    "print(\"p_idk\", p_idk)\n",
    "\n",
    "\n",
    "p_yes = 0\n",
    "yes_idx = tokenizer('yes', return_tensors=\"pt\").input_ids[0][0]\n",
    "for i in range(min(outputs['logits'].size(1), 4)):\n",
    "    p_yes += T.exp(outputs['logits'][0][i][yes_idx])\n",
    "print('p_yes', p_yes)\n",
    "\n",
    "'''\n",
    "\n",
    "p_no = 0\n",
    "for word in negation_list:   \n",
    "    word_idx = tokenizer(word, return_tensors=\"pt\").input_ids[0][:-1]\n",
    "    p_no = 1\n",
    "    print(word_idx)\n",
    "\n",
    "        \n",
    "first_word_logits = T.unsqueeze(outputs['logits'].view(-1, outputs['logits'].size(-1))[0],  dim=0)\n",
    "rest_logits = outputs['logits'].view(-1, outputs['logits'].size(-1))\n",
    "\n",
    "first_word_labels = T.unsqueeze(labels.view(-1)[0], dim=0)\n",
    "rest_labels = labels.view(-1)\n",
    "\n",
    "\n",
    "gen_len = labels.size(-1)\n",
    "\n",
    "first_word_weight = 0\n",
    "loss = first_word_weight * F.cross_entropy(first_word_logits, \n",
    "                    first_word_labels, \n",
    "                    ignore_index=-100) \\\n",
    "                + F.cross_entropy(rest_logits, \n",
    "                    rest_labels, \n",
    "                    ignore_index=-100)\n",
    "\n",
    "print(outputs['loss'])\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permute answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "name2col = {\n",
    "    'ref': 'answer1',\n",
    "    't5': 'answer2',\n",
    "    'ru': 'answer3',\n",
    "}\n",
    "\n",
    "df = pd.read_csv('10.csv')\n",
    "rand_perm_list = []\n",
    "for iter, row in df.iterrows():\n",
    "    a1 = row['answer1']\n",
    "    a2 = row['answer2']\n",
    "    a3 = row['answer3']\n",
    "    rand_perm = np.random.permutation(['ref', 't5', 'ru'])\n",
    "    df.at[iter, 'answer1'] = row[name2col[rand_perm[0]]]\n",
    "    df.at[iter, 'answer2'] = row[name2col[rand_perm[1]]]\n",
    "    df.at[iter, 'answer3'] = row[name2col[rand_perm[2]]]\n",
    "    rand_perm_list.append(rand_perm)\n",
    "\n",
    "df.to_csv('randomized_10.csv')\n",
    "with open('rand_perm_list', 'w') as f:\n",
    "    for rdp in rand_perm_list:\n",
    "        for name in rdp:\n",
    "            f.write(name)\n",
    "            f.write('\\t')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert clariq facet_id to qulac facet_id to reuse qulac qrel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "qulac_test_df = pd.read_json('cosearcher/data/qulac.json')\n",
    "print(qulac_test_df.columns)\n",
    "\n",
    "for iter, row in qulac_test_df.iterrows():\n",
    "    qulac_test_df.at[iter, 'facet_desc'] = re.sub('\\\\\\\\', '', qulac_test_df.at[iter, 'facet_desc'])\n",
    "\n",
    "def find_facet_id(facet):\n",
    "    for iter, row in qulac_test_df.iterrows():\n",
    "        if facet == qulac_test_df.at[iter, 'facet_desc']:\n",
    "            return qulac_test_df.at[iter, 'facet_id']\n",
    "    print(facet)\n",
    "    return 'error'\n",
    "\n",
    "clariq_test_df = pd.read_csv('clariq_dev.csv')\n",
    "clariq_test_df = clariq_test_df[clariq_test_df.topic_id < 201]\n",
    "for iter,row in clariq_test_df.iterrows():\n",
    "    clariq_test_df.at[iter, 'topic'] = clariq_test_df.at[iter, 'topic_desc']\n",
    "    clariq_test_df.at[iter, 'facet_id'] = find_facet_id(clariq_test_df.at[iter, 'facet_desc'])\n",
    "clariq_test_df = clariq_test_df[['topic','topic_desc','answer','facet_desc','question','facet_id','topic_id']].copy(deep=True)\n",
    "print(clariq_test_df.head(5))\n",
    "clariq_test_df.to_json('cosearcher/data/clariq.dev.json')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import f\n",
    "\n",
    "def TwoSampleT2Test(X, Y):\n",
    "    nx, p = X.shape\n",
    "    ny, _ = Y.shape\n",
    "    delta = np.mean(X, axis=0) - np.mean(Y, axis=0)\n",
    "    Sx = np.cov(X, rowvar=False)\n",
    "    Sy = np.cov(Y, rowvar=False)\n",
    "    S_pooled = ((nx-1)*Sx + (ny-1)*Sy)/(nx+ny-2)\n",
    "    t_squared = (nx*ny)/(nx+ny) * np.matmul(np.matmul(delta.transpose(), np.linalg.inv(S_pooled)), delta)\n",
    "    statistic = t_squared * (nx+ny-p-1)/(p*(nx+ny-2))\n",
    "    F = f(p, nx+ny-p-1)\n",
    "    p_value = 1 - F.cdf(statistic)\n",
    "    print(f\"Test statistic: {statistic}\\nDegrees of freedom: {p} and {nx+ny-p-1}\\np-value: {p_value}\")\n",
    "    return statistic, p_value\n",
    "\n",
    "\n",
    "ban_list = ['A1IU5OP7BBZHZ7',\n",
    "            'A299J4PKHAEU9H', # evidently not paying attention\n",
    "            'A26ZENZ5G8AEGM', # evidently not paying attention\n",
    "            'A9MYC5IGQ2DO4', \n",
    "            'A1XH05IKC77OXO',\n",
    "            'A2BAQ26SMQQEUG',\n",
    "            'AORHXBTOCXFUK',\n",
    "            'A26399B1QZ7XJJ',\n",
    "            'A17D6BK59S31BM',\n",
    "            'A13XXMDHOULEZ7',\n",
    "            'A1XO6ONCCTBMKW',\n",
    "            'A1EX0MEOPF8AHT',\n",
    "            'A28A3HF3LSEIDT',]\n",
    "            \n",
    "new_ban_list = [\n",
    "    'A2QQY4S73JO639',\n",
    "    'A273DS7TQWR9M1',\n",
    "    'A3TUMZ954ORSUC',\n",
    "    'A1BSOOHNHX51RI',\n",
    "    'A26399B1QZ7XJJ',\n",
    "    'A3QI1RV4HQ9MOC',\n",
    "    'A26ZENZ5G8AEGM',\n",
    "    'A1EX0MEOPF8AHT',\n",
    "    'A13XXMDHOULEZ7',\n",
    "    'A17D6BK59S31BM',\n",
    "    'A299J4PKHAEU9H',\n",
    "    'A2BAQ26SMQQEUG',\n",
    "    'A3O81LHBBI8NPK',\n",
    "    'A9MYC5IGQ2DO4',\n",
    "    'AORHXBTOCXFUK'\n",
    "    ]\n",
    "\n",
    "white_list = [\n",
    "    'A14W0AXTJ3R19V',\n",
    "    'A2KLJKDG90K1PP',\n",
    "    'A2TBXASXZIRNNW',\n",
    "    'A3HNEYFOIJWPH1',\n",
    "    'A3I9XLIHPPWPN1',\n",
    "    'AA9V4NE8SOA4I',\n",
    "    'APGX2WZ59OWDN',\n",
    "\n",
    "]\n",
    "\n",
    "batch_n = 10\n",
    "time_lower_bound = 120\n",
    "\n",
    "'''for i in range(1, batch_n+1):\n",
    "    pilot_name = 'batch_results_' +str(i) + '.csv'\n",
    "    batch_df = pd.read_csv(pilot_name)\n",
    "\n",
    "\n",
    "    if i == 1:\n",
    "        df = batch_df.copy(deep=True)\n",
    "    else:\n",
    "        df = pd.concat([df, batch_df])\n",
    "\n",
    "df = df.reset_index(drop=True)'''\n",
    "\n",
    "df = pd.read_csv('output/all_results.csv').reset_index(drop=True)\n",
    "\n",
    "\n",
    "num_q = df.shape[0] // 5\n",
    "model_names = ['ref', 't5', 'ru', 'copy']\n",
    "performances = {k:{'rel':[], 'nat':[]} for k in model_names}\n",
    "permutations = [l.strip().split('\\t') for l in open('new_rand_perm_list_200','r').readlines()[:num_q]]\n",
    "\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    ith_example = int(i/5)\n",
    "    results = row['Answer.taskAnswers']\n",
    "    result_dict = json.loads(results)[0]\n",
    "    for k,v in result_dict['ans1nat'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans1nat'] = score\n",
    "            df.at[i, permutations[ith_example][0]+'-nat'] = score\n",
    "\n",
    "    for k,v in result_dict['ans1rel'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans1rel'] = score\n",
    "            df.at[i, permutations[ith_example][0]+'-rel'] = score\n",
    "\n",
    "    for k,v in result_dict['ans2nat'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans2nat'] = score\n",
    "            df.at[i, permutations[ith_example][1]+'-nat'] = score\n",
    "\n",
    "    for k,v in result_dict['ans2rel'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans2rel'] = score\n",
    "            df.at[i, permutations[ith_example][1]+'-rel'] = score\n",
    "\n",
    "    for k,v in result_dict['ans3nat'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans3nat'] = score\n",
    "            df.at[i, permutations[ith_example][2]+'-nat'] = score\n",
    "\n",
    "    for k,v in result_dict['ans3rel'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans3rel'] = score\n",
    "            df.at[i, permutations[ith_example][2]+'-rel'] = score\n",
    "\n",
    "    for k,v in result_dict['ans4nat'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans4nat'] = score\n",
    "            df.at[i, permutations[ith_example][3]+'-nat'] = score\n",
    "\n",
    "    for k,v in result_dict['ans4rel'].items():\n",
    "        if v:\n",
    "            score = int(k[-1])\n",
    "            df.at[i, 'ans4rel'] = score\n",
    "            df.at[i, permutations[ith_example][3]+'-rel'] = score\n",
    "\n",
    "\n",
    "'''\n",
    "sns.kdeplot(\n",
    "    data=df, x=\"waiting\", y=\"duration\", hue=\"kind\", fill=True,\n",
    ")\n",
    "'''\n",
    "\n",
    "for i in range(num_q):\n",
    "    qi_results = df[i*5: (i+1)*5]\n",
    "    qi_answer1_nat, qi_answer1_rel = [], []\n",
    "    qi_answer2_nat, qi_answer2_rel = [], []\n",
    "    qi_answer3_nat, qi_answer3_rel = [], []\n",
    "    qi_answer4_nat, qi_answer4_rel = [], []\n",
    "\n",
    "    for i, row in qi_results.iterrows():\n",
    "        # if row['WorkTimeInSeconds'] < time_lower_bound or row['WorkerId'] in new_ban_list:\n",
    "        # if row['WorkTimeInSeconds'] < time_lower_bound :\n",
    "        #    continue\n",
    "        qi_answer1_nat.append(row['ans1nat'])\n",
    "        qi_answer1_rel.append(row['ans1rel'])\n",
    "        qi_answer2_nat.append(row['ans2nat'])\n",
    "        qi_answer2_rel.append(row['ans2rel'])\n",
    "        qi_answer3_nat.append(row['ans3nat'])\n",
    "        qi_answer3_rel.append(row['ans3rel'])\n",
    "        qi_answer4_nat.append(row['ans4nat'])\n",
    "        qi_answer4_rel.append(row['ans4rel'])\n",
    "\n",
    "    if len(qi_answer1_rel) > 0:\n",
    "        performances[permutations[(i+1)//5-1][0]]['rel'].append(np.mean(qi_answer1_rel))\n",
    "        performances[permutations[(i+1)//5-1][0]]['nat'].append(np.mean(qi_answer1_nat))\n",
    "\n",
    "    if len(qi_answer2_rel) > 0:\n",
    "        performances[permutations[(i+1)//5-1][1]]['rel'].append(np.mean(qi_answer2_rel))\n",
    "        performances[permutations[(i+1)//5-1][1]]['nat'].append(np.mean(qi_answer2_nat))\n",
    "\n",
    "    if len(qi_answer3_rel) > 0:\n",
    "        performances[permutations[(i+1)//5-1][2]]['rel'].append(np.mean(qi_answer3_rel))\n",
    "        performances[permutations[(i+1)//5-1][2]]['nat'].append(np.mean(qi_answer3_nat))\n",
    "    \n",
    "    if len(qi_answer4_rel) > 0:\n",
    "        performances[permutations[(i+1)//5-1][3]]['rel'].append(np.mean(qi_answer4_rel))\n",
    "        performances[permutations[(i+1)//5-1][3]]['nat'].append(np.mean(qi_answer4_nat))\n",
    "    \n",
    "# Importing library\n",
    "\n",
    "print(f'\\tRelevance\\tNaturalness')\n",
    "for k in performances.keys():\n",
    "    print(f\"{k}\\t{round(np.mean(performances[k]['rel']),3)}\\t\\t{round(np.mean(performances[k]['nat']),3)}\")\n",
    "\n",
    "\n",
    "# Performing the paired sample t-test\n",
    "print(\"T5 and R+U relevance significance test: \", stats.ttest_rel(performances['ru']['rel'], performances['t5']['rel']))\n",
    "print(\"T5 and R+U naturalness significance test: \",stats.ttest_rel(performances['ru']['nat'], performances['t5']['nat']))\n",
    "print(\"Copy and R+U relevance significance test: \", stats.ttest_rel(performances['ru']['rel'], performances['copy']['rel']))\n",
    "print(\"Copy and R+U naturalness significance test: \",stats.ttest_rel(performances['ru']['nat'], performances['copy']['nat']))\n",
    "\n",
    "# Visualization\n",
    "rel_l = performances['t5']['rel'] + performances['ru']['rel']\n",
    "nat_l = performances['t5']['nat'] + performances['ru']['nat']\n",
    "models = ['T5'] * len(performances['t5']['nat']) + ['Type+UQA'] * len(performances['ru']['nat'])\n",
    "snsdf = pd.DataFrame(list(zip(rel_l, nat_l, models)),\n",
    "               columns =['relevance', 'naturalness', 'model'])\n",
    "sns.set(font_scale=2)\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "g = sns.JointGrid(data=snsdf, x=\"relevance\", y=\"naturalness\", hue='model', xlim=(-10, 5.1), ylim=(-10, 5.1))\n",
    "g.plot_joint(sns.kdeplot, alpha=0.25, fill=True)\n",
    "g.plot_marginals(sns.kdeplot)\n",
    "sns.scatterplot(data=snsdf, x=\"relevance\", y=\"naturalness\", hue=\"model\", style=\"model\", ax=g.ax_joint)\n",
    "plt.show()\n",
    "plt.savefig('humankde.png')\n",
    "\n",
    "# hotelling's t-test\n",
    "print('Hotellings t-test:')\n",
    "t5 = snsdf.loc[snsdf['model']=='T5'][['relevance', 'naturalness']].copy(deep=True)\n",
    "ru = snsdf.loc[snsdf['model']=='Type+UQA'][['relevance', 'naturalness']].copy(deep=True)\n",
    "\n",
    "TwoSampleT2Test(t5, ru)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person correlation between generation length and scores\n",
    "generation_lengths = []\n",
    "full_rel_l, full_nat_l = [], []\n",
    "for i, row in df.iterrows():\n",
    "    if i % 5 == 0:\n",
    "        generation_lengths.append(len(row['Input.reference'].split()))\n",
    "        generation_lengths.append(len(row['Input.facet'].split()))\n",
    "        generation_lengths.append(len(row['Input.t5-candidate'].split()))\n",
    "        generation_lengths.append(len(row['Input.ru-candidate'].split()))\n",
    "\n",
    "        full_rel_l.append(row['ref-rel'])\n",
    "        full_rel_l.append(row['copy-rel'])\n",
    "        full_rel_l.append(row['t5-rel'])\n",
    "        full_rel_l.append(row['ru-rel'])\n",
    "\n",
    "        full_nat_l.append(row['ref-nat'])\n",
    "        full_nat_l.append(row['copy-nat'])\n",
    "        full_nat_l.append(row['t5-nat'])\n",
    "        full_nat_l.append(row['ru-nat'])\n",
    "\n",
    "\n",
    "pearson_df = pd.DataFrame(list(zip(generation_lengths, full_rel_l, full_nat_l)),\n",
    "               columns =['length', 'relevance', 'naturalness'])\n",
    "\n",
    "corr, _ = pearsonr(pearson_df['length'].values.tolist(), pearson_df['relevance'].values.tolist())\n",
    "print('Length-relevance correlation: %.3f' % corr)\n",
    "corr, _ = pearsonr(pearson_df['length'].values.tolist(), pearson_df['naturalness'].values.tolist())\n",
    "print('Length-naturalness correlation: %.3f' % corr)\n",
    "\n",
    "sns.scatterplot(data=pearson_df, x=\"length\", y=\"relevance\")\n",
    "plt.show()\n",
    "sns.scatterplot(data=pearson_df, x=\"length\", y=\"naturalness\")\n",
    "plt.show()\n",
    "sns.scatterplot(data=pearson_df, x=\"relevance\", y=\"naturalness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.stats import f\n",
    "\n",
    "def TwoSampleT2Test(X, Y):\n",
    "    nx, p = X.shape\n",
    "    ny, _ = Y.shape\n",
    "    delta = np.mean(X, axis=0) - np.mean(Y, axis=0)\n",
    "    Sx = np.cov(X, rowvar=False)\n",
    "    Sy = np.cov(Y, rowvar=False)\n",
    "    S_pooled = ((nx-1)*Sx + (ny-1)*Sy)/(nx+ny-2)\n",
    "    t_squared = (nx*ny)/(nx+ny) * np.matmul(np.matmul(delta.transpose(), np.linalg.inv(S_pooled)), delta)\n",
    "    statistic = t_squared * (nx+ny-p-1)/(p*(nx+ny-2))\n",
    "    F = f(p, nx+ny-p-1)\n",
    "    p_value = 1 - F.cdf(statistic)\n",
    "    print(f\"Test statistic: {statistic}\\nDegrees of freedom: {p} and {nx+ny-p-1}\\np-value: {p_value}\")\n",
    "    return statistic, p_value\n",
    "\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "versicolor = iris.data[iris.target==1, :2]\n",
    "virginica = iris.data[iris.target==2, :2]\n",
    "\n",
    "TwoSampleT2Test(versicolor, virginica)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new evaluation heuristics Qulac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "device = T.device(\"cuda\")\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"output/roberta-qulac/checkpoint-102\")\n",
    "\n",
    "local_dir = \"./output/unifiedqa-small-qulac-30-long/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_dir).cuda()\n",
    "df = pd.read_csv('qulac_test_long.csv')\n",
    "\n",
    "for iter, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    test_tokenized = tokenizer.encode_plus(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "    test_input_ids = test_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "\n",
    "    clf_inputs = roberta_tokenizer(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "\n",
    "    with T.no_grad():\n",
    "        logits = roberta_model(**clf_inputs).logits\n",
    "\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    prefix = ''\n",
    "    if predicted_class_id == 3 : # 3: yes\n",
    "        prefix = 'yes'\n",
    "    elif predicted_class_id == 1: #1: no\n",
    "        prefix = 'no'\n",
    "    elif predicted_class_id == 0: # 0: idk\n",
    "        prefix = 'i dont know'\n",
    "    else:\n",
    "        prefix = ''\n",
    "\n",
    "    tokenized_decoder_input = tokenizer.encode_plus(prefix, return_tensors=\"pt\")\n",
    "    decoder_input_ids = tokenized_decoder_input[\"input_ids\"].to(device)[0][0].item()\n",
    "    decoder_input_ids = T.tensor([[0, decoder_input_ids]]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    if prefix == '':\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            decoder_input_ids = decoder_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(beam_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "\n",
    "    df.at[iter, 'candidate'] = decoded_output.strip()\n",
    "    df.at[iter, 'facet_desc'] = df.at[iter, 'facet_desc'].lower()\n",
    "    df.at[iter, 'question'] = df.at[iter, 'question'].lower()\n",
    "\n",
    "    output = df[['facet_desc','question','answer','candidate']].copy(deep=True)\n",
    "    output.columns = ['facet', 'question', 'reference','candidate']\n",
    "    output.to_csv('output/unifiedqa-small-qulac-roberta-long.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new evaluation heuristic ClariQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "device = T.device(\"cuda\")\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"output/roberta-clariq/checkpoint-112\")\n",
    "\n",
    "local_dir = \"./output/unifiedqa-small-clariq-30-long/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_dir).cuda()\n",
    "df = pd.read_csv('clariq_dev_long.csv')\n",
    "\n",
    "for iter, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    test_tokenized = tokenizer.encode_plus(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "    test_input_ids = test_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "\n",
    "    clf_inputs = roberta_tokenizer(row['unifiedqa-question'], return_tensors=\"pt\")\n",
    "\n",
    "    with T.no_grad():\n",
    "        logits = roberta_model(**clf_inputs).logits\n",
    "\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    prefix = ''\n",
    "    if predicted_class_id == 3 : # 3: yes\n",
    "        prefix = 'yes'\n",
    "    elif predicted_class_id == 1: #1: no\n",
    "        prefix = 'no'\n",
    "    elif predicted_class_id == 0: # 0: idk\n",
    "        prefix = 'i dont know'\n",
    "    else:\n",
    "        prefix = ''\n",
    "\n",
    "    tokenized_decoder_input = tokenizer.encode_plus(prefix, return_tensors=\"pt\")\n",
    "    decoder_input_ids = tokenized_decoder_input[\"input_ids\"].to(device)[0][0].item()\n",
    "    decoder_input_ids = T.tensor([[0, decoder_input_ids]]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    if prefix == '':\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        beam_output = model.generate(\n",
    "            input_ids=test_input_ids,\n",
    "            decoder_input_ids = decoder_input_ids,\n",
    "            max_length=96,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            top_k=50, \n",
    "            top_p=0.9, \n",
    "            num_return_sequences=10,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(beam_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "\n",
    "    df.at[iter, 'candidate'] = decoded_output.strip()\n",
    "    df.at[iter, 'facet_desc'] = df.at[iter, 'facet_desc'].lower()\n",
    "    df.at[iter, 'question'] = df.at[iter, 'question'].lower()\n",
    "\n",
    "    output = df[['facet_desc','question','answer','candidate']].copy(deep=True)\n",
    "    output.columns = ['facet', 'question', 'reference','candidate']\n",
    "    output.to_csv('output/unifiedqa-small-clariq-roberta-long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import rouge\n",
    "rs = rouge.Rouge()\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "pos_tagger = spacy.load('en_core_web_sm')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "metric = evaluate.combine([\"rouge\"])\n",
    "\n",
    "def compute_metrics(refs, cands):\n",
    "    result = metric.compute(predictions=cands, references=refs, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items() }\n",
    "    return result['rougeL']\n",
    "\n",
    "df = pd.read_csv(\"output/t5-small-qulac-30-all.csv\")\n",
    "for i, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    df.at[i, 'low'] = compute_metrics(refs = [row['reference']], cands = [row['candidate']])\n",
    "\n",
    "df.to_csv('t-qulac-dev-study.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the qulac short and qulac long results and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# UnifiedQA\n",
    "qulac_long_gen_file = 'output/u-small-qulac-long/generated_predictions.txt'\n",
    "qulac_long_gen = open(qulac_long_gen_file).readlines()\n",
    "qulac_long_data_file = 'qulac_test_long.csv'\n",
    "qulac_long_df = pd.read_csv(qulac_long_data_file)\n",
    "qulac_short_gen_file = 'output/u-small-qulac-short/generated_predictions.txt'\n",
    "qulac_short_gen = open(qulac_short_gen_file).readlines()\n",
    "qulac_short_data_file = 'qulac_test_short.csv'\n",
    "qulac_short_df = pd.read_csv(qulac_short_data_file)\n",
    "\n",
    "for iter, row in qulac_long_df.iterrows():\n",
    "    qulac_long_df.at[iter, 'candidate'] = qulac_long_gen[iter].strip().lower()\n",
    "    qulac_long_df.at[iter, 'facet_desc'] = qulac_long_df.at[iter, 'facet_desc'].lower()\n",
    "    qulac_long_df.at[iter, 'question'] = qulac_long_df.at[iter, 'question'].lower()\n",
    "\n",
    "for iter, row in qulac_short_df.iterrows():\n",
    "    qulac_short_df.at[iter, 'candidate'] = qulac_short_gen[iter].strip().lower()\n",
    "    qulac_short_df.at[iter, 'facet_desc'] = qulac_short_df.at[iter, 'facet_desc'].lower()\n",
    "    qulac_short_df.at[iter, 'question'] = qulac_short_df.at[iter, 'question'].lower()\n",
    "\n",
    "qulac_full_df = pd.concat([qulac_short_df, qulac_long_df], ignore_index=True)\n",
    "qulac_full_df.to_csv('output/u-small-qulac-full.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClariQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# UnifiedQA\n",
    "qulac_long_gen_file = 'output/u-small-clariq-long/generated_predictions.txt'\n",
    "qulac_long_gen = open(qulac_long_gen_file).readlines()\n",
    "qulac_long_data_file = 'clariq_dev_long.csv'\n",
    "qulac_long_df = pd.read_csv(qulac_long_data_file)\n",
    "qulac_short_gen_file = 'output/u-small-clariq-short/generated_predictions.txt'\n",
    "qulac_short_gen = open(qulac_short_gen_file).readlines()\n",
    "qulac_short_data_file = 'clariq_dev_short.csv'\n",
    "qulac_short_df = pd.read_csv(qulac_short_data_file)\n",
    "\n",
    "for iter, row in qulac_long_df.iterrows():\n",
    "    qulac_long_df.at[iter, 'candidate'] = qulac_long_gen[iter].strip().lower()\n",
    "    qulac_long_df.at[iter, 'facet_desc'] = qulac_long_df.at[iter, 'facet_desc'].lower()\n",
    "    qulac_long_df.at[iter, 'question'] = qulac_long_df.at[iter, 'question'].lower()\n",
    "\n",
    "for iter, row in qulac_short_df.iterrows():\n",
    "    qulac_short_df.at[iter, 'candidate'] = qulac_short_gen[iter].strip().lower()\n",
    "    qulac_short_df.at[iter, 'facet_desc'] = qulac_short_df.at[iter, 'facet_desc'].lower()\n",
    "    qulac_short_df.at[iter, 'question'] = qulac_short_df.at[iter, 'question'].lower()\n",
    "\n",
    "qulac_full_df = pd.concat([qulac_short_df, qulac_long_df], ignore_index=True)\n",
    "qulac_full_df.to_csv('output/u-small-clariq-full.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate under new cooperativeness partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|         qulac          |                 generation similarity                |            retrieval performance            |\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL','BLEU3','BLEU4', 'ROUGE-L','METEOR', 'BERT-F1', 'TYPE-F1', 'nDCG@1', 'nDCG@5', 'nDCG@20', 'P@1', 'MRR'))\n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/t5-11b-roberta-qulac.csv', 'candidate', 'output/t5-11b-roberta-qulac.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-11b-roberta-qulac', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/u-11b-roberta-qulac.csv', 'candidate', 'output/u-11b-roberta-qulac.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('U-11b-roberta-qulac', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/f-11b-roberta-qulac.csv', 'candidate', 'output/f-11b-roberta-qulac.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('F-11b-roberta-qulac', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))  \n",
    "                                                                    \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/f-11b-roberta-qulac-noprompt.csv', 'candidate', 'output/f-11b-roberta-qulac-noprompt.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('F-11b-roberta-qulac-0', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClariQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|         qulac          |                 generation similarity                |            retrieval performance            |\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL','BLEU3','BLEU4', 'ROUGE-L','METEOR', 'BERT-F1', 'TYPE-F1', 'nDCG@1', 'nDCG@5', 'nDCG@20', 'P@1', 'MRR'))\n",
    "\n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/t5-11b-roberta-clariq.csv', 'candidate', 'output/t5-11b-roberta-clariq.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('T5-11b-roberta-clariq', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "\n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/u-11b-roberta-clariq.csv', 'candidate', 'output/u-11b-roberta-clariq.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('U-11b-roberta-clariq', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "           \n",
    "b3, b4, r, m, bf1, t, ndcg1, ndcg5, ndcg20, p1, mrr = evaluate_from_output('output/f-11b-roberta-clariq.csv', 'candidate', 'output/f-11b-roberta-clariq.json')\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"|{:<24}| {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}| {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('F-11b-roberta-clariq', \n",
    "                                                            round(b3, 2), round(b4, 2), round(r, 2), round(m, 2),round(bf1, 2),  round(np.mean(t)*100, 2),\n",
    "                                                            round(ndcg1, 4), round(ndcg5, 4), round(ndcg20, 4), round(p1, 4), round(mrr, 4),\n",
    "                                                            ))   \n",
    "                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3714/3714 [00:00<00:00, 4111.31it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4165.88it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4017.83it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4180.07it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4190.23it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4193.36it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4213.46it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4044.16it/s]\n",
      "100%|██████████| 2034/2034 [00:00<00:00, 4145.19it/s]\n",
      "100%|██████████| 2034/2034 [00:00<00:00, 3729.91it/s]\n",
      "100%|██████████| 2034/2034 [00:00<00:00, 4252.35it/s]\n",
      "100%|██████████| 2034/2034 [00:00<00:00, 4160.19it/s]\n",
      "100%|██████████| 2034/2034 [00:00<00:00, 4120.07it/s]\n",
      "100%|██████████| 2034/2034 [00:00<00:00, 4197.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "file_list = [\n",
    "    'output/gpt35-qulac-0shot.csv',\n",
    "    'output/gpt35-qulac-3shot.csv',\n",
    "    'output/gpt4-qulac-0shot.csv',\n",
    "    'output/gpt4-qulac-3shot.csv',\n",
    "    'output/llama2-qulac-0shot.csv',\n",
    "    'output/llama2-qulac-3shot.csv',\n",
    "    'output/flan-qulac-0shot.csv',\n",
    "    'output/flan-qulac-3shot.csv',\n",
    "\n",
    "    'output/gpt35-clariq-0shot.csv',\n",
    "    'output/gpt35-clariq-3shot.csv',\n",
    "    'output/gpt4-clariq-0shot.csv',\n",
    "    'output/gpt4-clariq-3shot.csv',\n",
    "    'output/llama2-clariq-0shot.csv',\n",
    "    'output/llama2-clariq-3shot.csv',\n",
    "    'output/flan-clariq-0shot.csv',\n",
    "    'output/flan-clariq-3shot.csv'\n",
    "]\n",
    "\n",
    "for fi in file_list:\n",
    "    try:\n",
    "        df = pd.read_csv(fi)\n",
    "    except:\n",
    "        continue\n",
    "    for iter, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "        df.at[iter, 'facet_desc'] = df.at[iter, 'facet_desc'].lower()\n",
    "        df.at[iter, 'question'] = df.at[iter, 'question'].lower()\n",
    "        try:\n",
    "            df.at[iter, 'candidate'] = re.sub('[^a-zA-Z0-9 -]', '', df.at[iter, 'candidate']).lower()\n",
    "            df.at[iter, 'candidate'] = re.sub('user response ', '', df.at[iter, 'candidate']).lower()\n",
    "        except:\n",
    "            pass\n",
    "    df.to_csv(fi, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
